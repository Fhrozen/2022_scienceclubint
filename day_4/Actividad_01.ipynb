{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645cb88e",
   "metadata": {},
   "source": [
    "# Computer vision and Sound Processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812edd2e",
   "metadata": {},
   "source": [
    "Refs:\n",
    "- https://pytorch.org/docs/stable/torch.html\n",
    "- https://deeplearning.neuromatch.io/tutorials/intro.html\n",
    "- https://github.com/alibaba/EasyCV\n",
    "- https://github.com/lucidrains/DALLE-pytorch\n",
    "- https://github.com/espnet/\n",
    "- https://github.com/speechbrain/speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8867d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37465ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, idim, odim):\n",
    "        super().__init__()\n",
    "        self.capa1 = \n",
    "        self.capa2 = \n",
    "    \n",
    "    def forward(self, tensor_x):\n",
    "        return tensor_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "train_set = \n",
    "test_set = \n",
    "\n",
    "train_kwargs = {'batch_size': 16}\n",
    "test_kwargs = {'batch_size': 16}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **test_kwargs)\n",
    "\n",
    "lr = \n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58826deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40bd0993",
   "metadata": {},
   "source": [
    "## Setting the training rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fb837",
   "metadata": {},
   "source": [
    "We import the optimizers to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = \n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c901ee6",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd848ff",
   "metadata": {},
   "source": [
    "Lets setup the training routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ce0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = \n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d0b75",
   "metadata": {},
   "source": [
    "You can now save your model with:\n",
    "```python\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe494d",
   "metadata": {},
   "source": [
    "## Now, you.\n",
    "\n",
    "Perform the same process (from the data loading up to the training), using the CIFAR10 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca3614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c9f745",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad0d2a",
   "metadata": {},
   "source": [
    "Use the model trained with MNIST and finetune it for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c6478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d097687b",
   "metadata": {},
   "source": [
    "# Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9b7c0",
   "metadata": {},
   "source": [
    "![](rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d335e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b78087",
   "metadata": {},
   "source": [
    "# Attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be71d5e",
   "metadata": {},
   "source": [
    "![](attn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bc563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01822139",
   "metadata": {},
   "source": [
    "# Semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92260a",
   "metadata": {},
   "source": [
    "![](semi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb50711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25e37ad8",
   "metadata": {},
   "source": [
    "# Self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a8089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1fc2fa",
   "metadata": {},
   "source": [
    "# Knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672dc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "551e9becf8fe707a6e42af77a66725df66af1e35803eb56e993681e98828e3c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
